// Hey! This is our main blog helper that manages all the P2P blog stuff
// We use these for handling binary data, file storage, and events
const b4a = require('b4a')
const Hyperdrive = require('hyperdrive')
const EventEmitter = require('events')

// simple state management
const state = {
  store: null,
  blog_core: null,
  drive: null,
  my_posts: [],
  peer_blogs: new Map(),
  discovered_blogs: new Map(),
  processed_posts: new Set(),
  emitter: new EventEmitter(),
  processing_peers: new Set() // Add this to prevent concurrent processing
}

// Initialize the blog system
async function init_blog(store, username) {
  state.store = store
  state.blog_core = store.get({ name: 'blog-feed' })
  state.drive = new Hyperdrive(store)
  
  await Promise.all([state.blog_core.ready(), state.drive.ready()])
  
  // Creating initial blog entry if new
  if (state.blog_core.length === 0) {
    await state.blog_core.append(JSON.stringify({
      type: 'blog-init',
      drive_key: b4a.toString(state.drive.key, 'hex'),
      title: `${username}'s Blog`,
      username,
      mode: typeof window === 'undefined' ? 'native' : 'browser',
      created: Date.now()
    }))
  }
  
  store.on('core', handle_new_core)
  state.blog_core.on('append', load_posts)
  
  await load_posts()
  await load_subscriptions()
  
  return { get_my_posts, get_peer_blogs, get_discovered_blogs }
}

// handle new peers
async function handle_new_core(core) {
  const key = b4a.toString(core.key, 'hex')
  if (key === b4a.toString(state.blog_core.key, 'hex')) return
  
  try {
    if (core.length > 0) {
      // first check if this is a blog init
      const init_data = JSON.parse(b4a.toString(await core.get(0)))
      if (init_data.type === 'blog-init' && init_data.username && init_data.title && (init_data.drive_key || init_data.driveKey)) {
        // Store or update blog info
        const blog = state.discovered_blogs.get(key) || {
          core,
          posts: [],
          processed_indices: new Set()
        }
        
        blog.last_seen = Date.now()
        blog.drive_key = init_data.drive_key || init_data.driveKey
        blog.title = init_data.title
        blog.username = init_data.username
        blog.mode = init_data.mode || 'unknown'
        
        // Make sure we store this in discovered blogs
        state.discovered_blogs.set(key, blog)
        console.log('Found initialized blog:', blog.username, 'Mode:', blog.mode)
        
        // Emit update when we discover a peer
        state.emitter.emit('update')
      }
      
      // check for new posts if were subscribed
      if (state.peer_blogs.has(key)) {
        await load_peer_posts(key)
        state.emitter.emit('update')
      }
    }
  } catch (err) {
    console.error('Core processing error:', err)
  }
}

// create and publish a post
async function create_post(title, content) {
  try {
    const path = `/posts/${Date.now()}.md`
    await state.drive.put(path, b4a.from(content))
    await state.blog_core.append(JSON.stringify({
      type: 'blog-post',
      title,
      filepath: path,
      created: Date.now()
    }))
    state.emitter.emit('update')
    return true
  } catch (err) {
    return false
  }
}

// subscribe to a peer's blog
async function subscribe(key) {
  if (state.peer_blogs.has(key)) return true
  
  try {
    const peer_core = state.discovered_blogs.get(key)?.core
    if (!peer_core || peer_core.length === 0) return false
    
    const data = JSON.parse(b4a.toString(await peer_core.get(0)))
    if (!data.drive_key || data.type !== 'blog-init') return false
    
    // create a new hyperdrive for this peer blogs
    const peer_drive = new Hyperdrive(state.store, b4a.from(data.drive_key, 'hex'))
    await peer_drive.ready()
    
    // store subscription info in our core for persistence
    await state.blog_core.append(JSON.stringify({
      type: 'peer-blog-init',
      peer_key: key,
      username: data.username,
      title: data.title,
      drive_key: data.drive_key,
      subscribed_at: Date.now()
    }))
    
    // store peer info in our state
    state.peer_blogs.set(key, {
      core: peer_core,
      drive: peer_drive,
      posts: [],
      title: data.title,
      username: data.username,
      processed_indices: new Set() // initializing the processed indices 
    })
    
    // replication for this peers core and drive
    peer_core.download({ start: 0, end: -1 }) // Download all existing blocks
    peer_drive.download('/') // Download all drive content
    
    // listen for future updates, but get off the hook to prevent rapid-fire calls
    let update_timeout = null
    peer_core.on('append', () => {
      if (update_timeout) clearTimeout(update_timeout)
      update_timeout = setTimeout(() => load_peer_posts(key), 100) // 100ms get off
    })
    
    // load existing posts
    await load_peer_posts(key)
    
    save_subscriptions()
    state.emitter.emit('update')
    return true
  } catch (err) {
    console.error('Subscribe error:', err)
    return false
  }
}

// unsubscribe from a blog
function unsubscribe(key) {
  const removed = state.peer_blogs.delete(key)
  if (removed) {
    save_subscriptions()
    state.emitter.emit('update')
  }
  return removed
}

// load all posts (ours and peers)
async function load_posts() {
  state.my_posts = []
  state.processed_posts.clear()
  
  for (let i = 1; i < state.blog_core.length; i++) {
      try {
      const entry = JSON.parse(b4a.toString(await state.blog_core.get(i)))
      
      if (entry.type === 'blog-post') {
        const content = await state.drive.get(entry.filepath)
        if (content) {
          const contentText = b4a.toString(content)
          state.my_posts.push({ ...entry, content: contentText })
        }
      }
      // handle peer blog init entries to restore peer info
      else if (entry.type === 'peer-blog-init') {
        const { peer_key, username, title, drive_key } = entry
        if (!state.peer_blogs.has(peer_key)) {
          const peer_core = state.store.get(b4a.from(peer_key, 'hex'))
          const peer_drive = new Hyperdrive(state.store, b4a.from(drive_key, 'hex'))
          await Promise.all([peer_core.ready(), peer_drive.ready()])
          
          state.peer_blogs.set(peer_key, {
            core: peer_core,
            drive: peer_drive,
            posts: [],
            title,
            username,
            processed_indices: new Set() // Initialize here too
          })
          
          // Set up listener for future updates with debounce
          let update_timeout = null
          peer_core.on('append', () => {
            if (update_timeout) clearTimeout(update_timeout)
            update_timeout = setTimeout(() => load_peer_posts(peer_key), 100)
          })
        }
      }
      // Handle stored peer posts
      else if (entry.type === 'peer-blog-post') {
        const { peer_key, title, filepath, created } = entry
        const post_id = `${peer_key}:${title}:${created}`
        state.processed_posts.add(post_id)
        
        const peer = state.peer_blogs.get(peer_key)
        if (peer) {
          const content = await state.drive.get(filepath)
          if (content) {
            const contentText = b4a.toString(content)
            if (!peer.posts) peer.posts = []
            if (!peer.posts.some(p => p.title === title && p.created === created)) {
              peer.posts.push({ title, content: contentText, created, filepath })
            }
          }
        }
    }
  } catch (err) {
      console.error('error loading post:', err)
    }
  }
}

// Load peer posts but not that we have already processed
async function load_peer_posts(key) {
  if (state.processing_peers.has(key)) {
    console.log(`Already processing peer ${key}, skipping...`)
    return
  }
  
  const peer = state.peer_blogs.get(key)
  if (!peer) return
  
  state.processing_peers.add(key)
  
  try {
    if (!peer.processed_indices) {
      peer.processed_indices = new Set()
    }
    
    for (let i = 1; i < peer.core.length; i++) {
      // Skip if we've already processed this index
      if (peer.processed_indices.has(i)) continue
      
      try {
        const post = JSON.parse(b4a.toString(await peer.core.get(i)))
      
        // only process direct blog posts from this peer, not their replicated posts
        // we dont want indirect posts
        if (post.type === 'blog-post') {
          const post_id = `${key}:${post.title}:${post.created}`
          
          // again check we haven't processed this exact post
          if (!state.processed_posts.has(post_id)) {
            try {
              const content = await peer.drive.get(post.filepath)
              if (content) {
                const contentText = b4a.toString(content)
                const our_path = `/peer_posts/${key}/${post.filepath.split('/').pop()}`
                
                // store in our drive
                await state.drive.put(our_path, content)
                
                // record in our core
                await state.blog_core.append(JSON.stringify({
                  type: 'peer-blog-post',
                  peer_key: key,
                  title: post.title,
                  filepath: our_path,
                  created: post.created
                }))
                
                // sdd to peer's posts if not already there
                if (!peer.posts) peer.posts = []
                if (!peer.posts.some(p => p.title === post.title && p.created === post.created)) {
                  peer.posts.push({
                    title: post.title,
                    content: contentText,
                    created: post.created,
                    filepath: our_path
                  })
                }
                
                state.processed_posts.add(post_id)
                console.log(`Processed new post: ${post.title} from ${key}`)
              }
            } catch (err) {
              console.warn('Error processig post content:', err)
            }
          } else {
            console.log(`Post already processed: ${post.title} from ${key}`)
          }
        }

        // Mark this index as processed even if it wasnt a blog post
        peer.processed_indices.add(i)
      } catch (err) {
        console.error('Error processing peer post:', err)
        // Still mark as processed to avoid infinite retry
        peer.processed_indices.add(i)
      }
    }
    
    state.emitter.emit('update')
  } finally {
    // always remove from processing set
    state.processing_peers.delete(key)
  }
}

// store subscriptions in local storage
function save_subscriptions() {
  if (typeof window !== 'undefined' && window.localStorage) {
  localStorage.setItem('blog_subscriptions', 
      JSON.stringify(Array.from(state.peer_blogs.keys())))
}
}

async function load_subscriptions() {
  try {
    if (typeof window !== 'undefined' && window.localStorage) {
      const subs = JSON.parse(localStorage.getItem('blog_subscriptions') || '[]')
      for (const key of subs) {
        const peer_core = state.store.get(b4a.from(key, 'hex'))
        state.discovered_blogs.set(key, { core: peer_core, last_seen: 0 })
      }
    }
  } catch (err) {}
}

// simple getters
const get_my_posts = () => {
  const unique_posts = new Map()
  state.my_posts.forEach(post => {
    const key = `${post.title}:${post.created}`
    unique_posts.set(key, post)
  })
  return Array.from(unique_posts.values())
}
const get_peer_blogs = () => state.peer_blogs
const get_discovered_blogs = () => state.discovered_blogs
const on_update = (cb) => state.emitter.on('update', cb)
const get_my_core_key = () => state.blog_core?.key // Get our core key for comparison

module.exports = {
  init_blog,
  handle_new_core,
  create_post,
  subscribe,
  unsubscribe,
  get_my_posts,
  get_peer_blogs,
  get_discovered_blogs,
  get_my_core_key,
  on_update
}